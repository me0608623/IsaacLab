---
name: charge-reward
description: è™•ç† Charge å°ˆæ¡ˆçš„çå‹µå‡½æ•¸ç›¸é—œä»»å‹™ï¼ŒåŒ…å«ç›®æ¨™çå‹µã€å®‰å…¨çå‹µã€é‹å‹•çå‹µçš„è¨­è¨ˆèˆ‡èª¿æ•´ã€‚åœ¨éœ€è¦ä¿®æ”¹æˆ–èª¿è©¦çå‹µå‡½æ•¸æ™‚ä½¿ç”¨ã€‚
---

# Charge å°ˆæ¡ˆ - çå‹µå‡½æ•¸å°ˆç”¨ Skill

## ğŸ çå‹µå‡½æ•¸æ¶æ§‹

### çå‹µå‡½æ•¸ä½ç½®
```
charge_sb3/mdp/rewards/
â”œâ”€â”€ goal_rewards.py       # ç›®æ¨™ç›¸é—œçå‹µ
â”œâ”€â”€ safety_rewards.py     # å®‰å…¨ç›¸é—œçå‹µ
â”œâ”€â”€ motion_rewards.py     # é‹å‹•ç›¸é—œçå‹µ
â””â”€â”€ utils.py              # çå‹µå·¥å…·å‡½æ•¸
```

### å°å…¥ä½ç½®
```python
# æ‰€æœ‰çå‹µå‡½æ•¸éƒ½å¾ mdp/rewards/__init__.py çµ±ä¸€å°å‡º
from ..mdp.rewards import (
    # ç›®æ¨™çå‹µ
    velocity_toward_goal,
    reaching_goal,
    progress_to_goal,
    # å®‰å…¨çå‹µ
    collision_penalty,
    obstacle_avoidance_reward,
    # é‹å‹•çå‹µ
    forward_velocity_reward,
    action_rate_penalty,
)
```

---

## ğŸ“‹ å¸¸ç”¨çå‹µå‡½æ•¸é€ŸæŸ¥

### ç›®æ¨™çå‹µ (goal_rewards.py)

| å‡½æ•¸ | æè¿° | å…¸å‹æ¬Šé‡ |
|------|------|:--------:|
| `velocity_toward_goal` | é€Ÿåº¦åœ¨ç›®æ¨™æ–¹å‘çš„æŠ•å½± | +1.0 ~ +3.0 |
| `reaching_goal` | æŠµé”ç›®æ¨™ï¼ˆè·é›¢ < 0.3mï¼‰| +500.0 |
| `progress_to_goal` | è·é›¢æ¸›å°‘é‡ (d_{t-1} - d_t) | +2.0 ~ +5.0 |
| `heading_to_goal` | æœå‘ç›®æ¨™çš„çå‹µ | +0.5 |
| `approaching_goal_bonus` | æ¥è¿‘ç›®æ¨™é¡å¤–çå‹µ | +1.0 |

### å®‰å…¨çå‹µ (safety_rewards.py)

| å‡½æ•¸ | æè¿° | å…¸å‹æ¬Šé‡ |
|------|------|:--------:|
| `collision_penalty` | ç¢°æ’æ‡²ç½° | -1.0 ~ -5.0 |
| `wall_collision_penalty` | ç‰†å£ç¢°æ’æ‡²ç½° | -10.0 |
| `obstacle_avoidance_reward` | é¿éšœçå‹µ | +0.1 ~ +0.5 |
| `progressive_collision_penalty` | æ¼¸é€²å¼ç¢°æ’æ‡²ç½° | -0.5 |
| `safe_navigation_bonus` | å®‰å…¨å°èˆªçå‹µ | +0.2 |

### é‹å‹•çå‹µ (motion_rewards.py)

| å‡½æ•¸ | æè¿° | å…¸å‹æ¬Šé‡ |
|------|------|:--------:|
| `forward_velocity_reward` | å‰é€²é€Ÿåº¦çå‹µ | +0.5 ~ +1.0 |
| `forward_motion_reward` | å‘å‰ç§»å‹•çå‹µ | +0.3 |
| `move_reward` | ç§»å‹•çå‹µï¼ˆéé›¶é€Ÿåº¦ï¼‰| +0.1 |
| `action_rate_penalty` | å‹•ä½œè®ŠåŒ–æ‡²ç½°ï¼ˆé˜²æŠ–ï¼‰| -0.01 ~ -0.1 |
| `time_out_penalty` | è¶…æ™‚æ‡²ç½° | -0.05 |

---

## ğŸ”§ æ¨™æº–ä¿®æ”¹æµç¨‹

### 1. å®šç¾©æ–°çå‹µå‡½æ•¸
```python
# åœ¨ mdp/rewards/your_rewards.py ä¸­
import torch
from isaaclab.envs import ManagerBasedEnv

def your_custom_reward(env: ManagerBasedEnv) -> torch.Tensor:
    """
    ä½ çš„è‡ªå®šç¾©çå‹µå‡½æ•¸

    Args:
        env: ç’°å¢ƒå¯¦ä¾‹

    Returns:
        torch.Tensor: çå‹µå€¼ï¼Œå½¢ç‹€ [num_envs]
    """
    # ç²å–éœ€è¦çš„æ•¸æ“š
    robot_state = env.robot.data.root_state_w  # [num_envs, 13]
    goal_pos = env.goal_buffer  # [num_envs, 3]

    # è¨ˆç®—çå‹µ
    reward = torch.zeros(env.num_envs, device=env.device)

    # ä½ çš„é‚è¼¯...
    reward = your_calculation(robot_state, goal_pos)

    return reward
```

### 2. å°å‡ºçå‹µå‡½æ•¸
```python
# åœ¨ mdp/rewards/__init__.py ä¸­
from .your_rewards import your_custom_reward

__all__ = [..., "your_custom_reward"]
```

### 3. åœ¨ç’°å¢ƒé…ç½®ä¸­ä½¿ç”¨
```python
# åœ¨ cfg/charge_env_cfg_*.py ä¸­
from ..mdp.rewards import your_custom_reward

@configclass
class RewardsCfg:
    your_reward_term = RewTerm(
        func=your_custom_reward,
        weight=1.0,
        params={}  # å¯é¸åƒæ•¸
    )
```

---

## âš–ï¸ çå‹µæ¬Šé‡è¨­è¨ˆåŸå‰‡

### æ¬Šé‡æ¯”ä¾‹å»ºè­°

**ç›®æ¨™ vs é¿éšœæ¯”ä¾‹**ï¼š
```
ç†æƒ³æ¯”ä¾‹ = ç›®æ¨™å°å‘çå‹µ : é¿éšœæ‡²ç½° â‰ˆ 25:1 ä»¥ä¸Š

# ç¤ºä¾‹ï¼š
velocity_toward_goal (weight=5.0)
collision_penalty (weight=-0.2)
æ¯”ä¾‹ = 5.0 / 0.2 = 25:1 âœ… å¥åº·çš„æ¯”ä¾‹
```

**çµ‚æ¥µçå‹µï¼ˆSparse Rewardï¼‰**ï¼š
```python
# æŠµé”ç›®æ¨™æ‡‰è©²çµ¦äºˆæ¥µå¤§çš„çå‹µ
reaching_goal = RewTerm(
    func=reaching_goal,
    weight=500.0,  # é å¤§æ–¼å…¶ä»–çå‹µ
)
```

### çå‹µæ­¸ä¸€åŒ–
```python
# ç¢ºä¿æ‰€æœ‰çå‹µåœ¨åŒä¸€æ•¸é‡ç´š
# å»ºè­°ç¯„åœï¼š[-10, +10] per step

# ä½¿ç”¨ ExpectedValueTracker è¿½è¹¤
from ..mdp.rewards import ExpectedValueTracker

tracker = ExpectedValueTracker()
tracker.update("your_reward", reward_values)
# å®šæœŸæ‰“å°æª¢æŸ¥
```

---

## ğŸ› å¸¸è¦‹å•é¡Œæ’æŸ¥

### å•é¡Œ 1ï¼šAgent è›‡è¡Œï¼ˆOscillationï¼‰
**ç—‡ç‹€**ï¼šAgent å·¦å³æ“ºå‹•ï¼Œç„¡æ³•ç›´è¡Œ

**è¨ºæ–·**ï¼š
```python
# æª¢æŸ¥ç›®æ¨™:é¿éšœæ¯”ä¾‹
goal_weight = 5.0
avoid_weight = 0.4
ratio = goal_weight / avoid_weight  # = 12.5:1 âŒ å¤ªä½

# è§£æ±ºï¼šå¢åŠ ç›®æ¨™æ¬Šé‡æˆ–é™ä½é¿éšœæ¬Šé‡
goal_weight = 10.0  # æé«˜åˆ° 10
# ratio = 10.0 / 0.4 = 25:1 âœ…
```

### å•é¡Œ 2ï¼šAgent ç¿»è»Šé¨™åˆ†ï¼ˆTipping Hackï¼‰
**ç—‡ç‹€**ï¼šAgent æ•…æ„ç¿»è»Šæ»‘è¡Œåˆ°ç›®æ¨™

**è§£æ±º**ï¼šä½¿ç”¨å§¿æ…‹é–€æ§
```python
# åœ¨é‹å‹•çå‹µä¸­åŠ å…¥å§¿æ…‹æª¢æŸ¥
def forward_velocity_reward_gated(env: ManagerBasedEnv) -> torch.Tensor:
    reward = forward_velocity_reward(env)

    # æª¢æŸ¥æ˜¯å¦æ­£ç«‹ï¼ˆz è»¸å‘ä¸Šï¼‰
    up_vector = env.robot.data.root_state_w[:, 6:9]  # [num_envs, 3]
    is_upright = up_vector[:, 2] > 0.5  # z åˆ†é‡ > 0.5

    # ç¿»è»Šå‰‡å–æ¶ˆçå‹µ
    reward = reward * is_upright.float()

    return reward
```

### å•é¡Œ 3ï¼šAgent åŸåœ°æ‰“è½‰ï¼ˆSpinningï¼‰
**ç—‡ç‹€**ï¼šAgent åŸåœ°æ—‹è½‰ä¸å‰é€²

**è§£æ±º**ï¼šæ·»åŠ å‰é€²çå‹µ
```python
class RewardsCfg:
    # çå‹µå‘å‰çš„é€Ÿåº¦åˆ†é‡
    forward_velocity = RewTerm(
        func=forward_velocity_reward,
        weight=0.5,
    )
    # æ‡²ç½°è§’é€Ÿåº¦éå¤§ï¼ˆè½‰å¤ªåœˆï¼‰
    action_rate_penalty = RewTerm(
        func=action_rate_penalty,
        weight=-0.05,
    )
```

---

## ğŸ“Š çå‹µå‡½æ•¸èª¿è©¦æŠ€å·§

### ä½¿ç”¨ Reward Checker
```python
# åœ¨ mdp/rewards/utils.py ä¸­æœ‰å…§å»ºçš„æª¢æŸ¥å·¥å…·
from ..mdp.rewards import _check_reward_term

# åœ¨è¨“ç·´è…³æœ¬ä¸­å•Ÿç”¨
export REWARD_CHECK=1  # ç’°å¢ƒè®Šé‡é–‹å•Ÿæª¢æŸ¥
```

### TensorBoard ç›£æ§
```python
# ç¢ºä¿çå‹µè¢«æ­£ç¢ºè¨˜éŒ„
with torch.no_grad():
    for name, reward_term in env.reward_manager terms:
        writer.add_scalar(f"rewards/{name}", reward_term.mean(), step)
```

---

## ğŸšï¸ Phase å°ˆç”¨çå‹µé…ç½®

### Phase 0ï¼ˆåŸºç¤é‹å‹•å­¸ï¼‰
```python
class RewardsCfg:
    # å°ˆæ³¨æ–¼ç§»å‹•å­¸ç¿’
    progress = RewTerm(func=progress_to_goal, weight=20.0)
    goal = RewTerm(func=reaching_goal, weight=10.0)
    collision = RewTerm(func=collision_penalty, weight=-10.0)
    smooth = RewTerm(func=action_rate_penalty, weight=-0.5)
```

### Phase 3ï¼ˆé•·ç¨‹å°èˆªï¼‰
```python
class RewardsCfg:
    # å°ˆæ³¨æ–¼ç›®æ¨™å°å‘
    reaching_goal = RewTerm(func=reaching_goal, weight=500.0)
    velocity_to_goal = RewTerm(func=velocity_toward_goal, weight=3.0)
    distance_progress = RewTerm(func=progress_to_goal, weight=5.0)
    tipped_over = RewTerm(func=tipped_over_penalty, weight=-200.0)
```
