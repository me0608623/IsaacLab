---
name: charge-debug
description: è™•ç† Charge å°ˆæ¡ˆçš„èª¿è©¦ä»»å‹™ï¼ŒåŒ…å«å¸¸è¦‹å•é¡Œè¨ºæ–·ã€éŒ¯èª¤æ’æŸ¥ã€è¨“ç·´å•é¡Œè§£æ±ºç­‰ã€‚åœ¨é‡åˆ°è¨“ç·´å•é¡Œæˆ–ç•°å¸¸è¡Œç‚ºæ™‚ä½¿ç”¨ã€‚
---

# Charge å°ˆæ¡ˆ - èª¿è©¦å°ˆç”¨ Skill

## ğŸ› å¸¸è¦‹å•é¡Œè¨ºæ–·æµç¨‹

### å•é¡Œè¨ºæ–·æª¢æŸ¥æ¸…å–®

```python
âœ… ç’°å¢ƒå°å…¥æ­£ç¢ºï¼Ÿ
âœ… è§€æ¸¬ç¶­åº¦åŒ¹é…ï¼Ÿ
âœ… çå‹µå‡½æ•¸è¿”å›æœ‰æ•ˆå€¼ï¼Ÿ
âœ… PPO è¶…åƒæ•¸åˆç†ï¼Ÿ
âœ… Episode é•·åº¦é©ç•¶ï¼Ÿ
âœ… çµ‚æ­¢æ¢ä»¶æ­£å¸¸å·¥ä½œï¼Ÿ
âœ… Isaac Sim ç‰ˆæœ¬å…¼å®¹ï¼Ÿ
```

---

## ğŸ”´ é«˜é »å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### å•é¡Œ 1ï¼šPPO StdDev = 0 éŒ¯èª¤

**éŒ¯èª¤è¨Šæ¯**ï¼š
```
RuntimeError: The standard deviation of the action distribution is zero.
This can happen when the action distribution is too deterministic.
```

**æ ¹æœ¬åŸå› **ï¼šè§€æ¸¬å€¼æ–¹å·®ç‚ºé›¶ï¼ˆæ‰€æœ‰å€¼ç›¸åŒï¼‰

**è¨ºæ–·æ­¥é©Ÿ**ï¼š
```python
# 1. æª¢æŸ¥è§€æ¸¬å€¼ç¯„åœ
obs, _ = env.reset()
print(f"Obs range: [{obs.min()}, {obs.max()}]")
print(f"Obs std: {obs.std(dim=0)}")

# 2. æª¢æŸ¥å„å€‹è§€æ¸¬åˆ†é‡
print(f"LiDAR mean: {obs[:, :120].mean()}, std: {obs[:, :120].std(dim=0)}")
print(f"Goal position: {obs[:, 120:122]}")
```

**å¸¸è¦‹åŸå› **ï¼š

| åŸå›  | ç—‡ç‹€ | ä¿®å¾©æ–¹æ³• |
|------|------|----------|
| LiDAR è¿”å›ç›¸åŒè·é›¢ | obs[0:120].std() â‰ˆ 0 | æª¢æŸ¥å‚³æ„Ÿå™¨é…ç½®ï¼Œæ·»åŠ éš¨æ©Ÿæ€§ |
| ç›®æ¨™ä½ç½®ç¸½æ˜¯ [0, 0] | goal ä¸è®Š | ä¿®å¾© goal_command.py |
| è§€æ¸¬æœªæ­¸ä¸€åŒ– | æ–¹å·®éå¤§ï¼Œæ¢¯åº¦å•é¡Œ | æ·»åŠ æ­¸ä¸€åŒ–åˆ° [0, 1] |

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```python
# è§£æ±ºæ–¹æ¡ˆ 1ï¼šçµ¦ LiDAR æ·»åŠ å™ªè²
lidar_raw = ray_caster.data.out_dist
lidar_noisy = lidar_raw + torch.randn_like(lidar_raw) * 0.01

# è§£æ±ºæ–¹æ¡ˆ 2ï¼šæª¢æŸ¥æ­¸ä¸€åŒ–
normalized = torch.clamp(raw_value / max_range, 0.0, 1.0)

# è§£æ±ºæ–¹æ¡ˆ 3ï¼šä½¿ç”¨è‡ªå®šç¾©ç’°å¢ƒé¡æª¢æŸ¥è§€æ¸¬
# åœ¨ cfg/charge_env.py ä¸­çš„ ChargeNavigationEnv
```

---

### å•é¡Œ 2ï¼šè›‡è¡Œè¡Œç‚ºï¼ˆSnake Behaviorï¼‰

**ç—‡ç‹€**ï¼š
- Agent åœ¨éšœç¤™ç‰©é™„è¿‘å·¦å³æ“ºå‹•
- è·¯å¾‘å‘ˆç¾æ­£å¼¦æ³¢æ¨¡å¼
- é€²åº¦é¡¯è‘—æ¸›æ…¢

**æ ¹æœ¬åŸå› åˆ†æ**ï¼š
```python
# è¨ˆç®—ç›®æ¨™:é¿éšœæ¯”ä¾‹
goal_weight = 5.0  # distance_to_goal
avoidance_weight = 0.4  # progressive_collision
ratio = goal_weight / avoidance_weight  # = 12.5:1

if ratio < 25:
    print(f"âŒ Snake behavior expected: ratio too low ({ratio:.2f}:1)")
else:
    print(f"âœ… Ratio healthy: {ratio:.2f}:1")
```

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šå¢åŠ ç›®æ¨™æ¬Šé‡
class RewardsCfg:
    velocity_to_goal = RewTerm(
        func=velocity_toward_goal,
        weight=10.0,  # å¾ 5.0 æé«˜åˆ° 10.0
    )

# æ–¹æ¡ˆ 2ï¼šä½¿ç”¨è·é›¢åŠ æ¬Š
# é è·é›¢æ™‚æ›´é—œæ³¨ç›®æ¨™ï¼Œè¿‘è·é›¢æ™‚æ›´é—œæ³¨é¿éšœ
class RewardsCfg:
    velocity_to_goal_dw = RewTerm(
        func=velocity_toward_goal_distance_weighted,
        weight=1.0,
        params={"far_weight": 2.0, "near_weight": 0.5},
    )

# æ–¹æ¡ˆ 3ï¼šæ·»åŠ æœå‘çå‹µ
class RewardsCfg:
    heading_to_goal = RewTerm(
        func=heading_to_goal_distance_weighted,
        weight=1.0,
    )
```

---

### å•é¡Œ 3ï¼šAgent ç¿»è»Šé¨™åˆ†ï¼ˆTipping Hackï¼‰

**ç—‡ç‹€**ï¼š
- Agent æ•…æ„ç¿»å€’
- ç¿»è»Šå¾Œæ»‘è¡Œåˆ°ç›®æ¨™
- åˆ©ç”¨ç‰©ç†æ¼æ´

**è¨ºæ–·**ï¼š
```python
# æª¢æŸ¥å§¿æ…‹åˆ†ä½ˆ
up_vector = env.robot.data.root_state_w[:, 6:9]  # [num_envs, 3]
uprightness = up_vector[:, 2]  # z åˆ†é‡

print(f"Upright ratio: {(uprightness > 0.5).float().mean()}")
# å¦‚æœ < 0.8ï¼Œèªªæ˜å¾ˆå¤š Agent åœ¨ç¿»è»Šç‹€æ…‹
```

**è§£æ±ºæ–¹æ¡ˆ - å§¿æ…‹é–€æ§**ï¼š
```python
def forward_velocity_reward_gated(env: ManagerBasedEnv) -> torch.Tensor:
    # åŸºç¤çå‹µ
    reward = forward_velocity_reward(env)

    # æª¢æŸ¥æ˜¯å¦æ­£ç«‹
    up_vector = env.robot.data.root_state_w[:, 6:9]
    is_upright = up_vector[:, 2] > 0.5  # z è»¸å‘ä¸Š

    # ç¿»è»Šå‰‡å–æ¶ˆæ‰€æœ‰ç§»å‹•çå‹µ
    reward = reward * is_upright.float()

    return reward

# åœ¨é…ç½®ä¸­ä½¿ç”¨
class RewardsCfg:
    forward_velocity = RewTerm(
        func=forward_velocity_reward_gated,  # ä½¿ç”¨é–€æ§ç‰ˆæœ¬
        weight=1.0,
    )
```

---

### å•é¡Œ 4ï¼šææ‡¼éšœç¤™ç‰©ï¼ˆFear of Obstaclesï¼‰

**ç—‡ç‹€**ï¼š
- Agent åœç•™åœ¨åŸåœ°ä¸æ•¢ç§»å‹•
- ç¹å¤§åœˆé é›¢éšœç¤™
- è¨“ç·´æ”¶æ–‚æ¥µæ…¢

**æ ¹æœ¬åŸå› **ï¼šç¢°æ’æ‡²ç½°éé«˜ï¼ŒAgent å­¸æœƒã€Œä¸åšä¸é”™ã€

**è§£æ±ºæ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ 1ï¼šé™ä½ç¢°æ’æ‡²ç½°
class RewardsCfg:
    collision_penalty = RewTerm(
        func=collision_penalty,
        weight=-0.5,  # å¾ -5.0 é™åˆ° -0.5
    )

# æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ¼¸é€²å¼æ‡²ç½°
class RewardsCfg:
    progressive_collision = RewTerm(
        func=progressive_collision_penalty,
        weight=-1.0,
        params={
            "near_threshold": 0.3,  # 0.3m å…§é–‹å§‹æ‡²ç½°
            "far_threshold": 1.0,   # 1.0m é”åˆ°æœ€å¤§
        },
    )

# æ–¹æ¡ˆ 3ï¼šPhase 3 ç­–ç•¥ï¼ˆç§»é™¤éšœç¤™ï¼‰
# å…ˆè¨“ç·´ç´”æ·¨çš„å°èˆªæ„å¿—ï¼Œå†åŠ å›éšœç¤™
class MySceneCfg:
    num_static_obstacles = 0  # Phase 3: ç„¡éšœç¤™ç‰©
```

---

### å•é¡Œ 5ï¼šè¨“ç·´ä¸æ”¶æ–‚

**è¨ºæ–·æª¢æŸ¥æ¸…å–®**ï¼š

```python
# 1. æª¢æŸ¥å­¸ç¿’ç‡
learning_rate = 3e-4  # æ¨™æº–å€¼
# å¦‚æœ < 1e-5: å¤ªå°ï¼Œæ”¶æ–‚æ…¢
# å¦‚æœ > 1e-3: å¤ªå¤§ï¼Œä¸ç©©å®š

# 2. æª¢æŸ¥çå‹µå°ºåº¦
total_reward = env.reward_manager.compute()
print(f"Total reward: mean={total_reward.mean():.2f}, std={total_reward.std():.2f}")
# æ­£å¸¸ç¯„åœï¼š[-50, +50] per episode
# å¦‚æœçµ•å°å€¼ > 1000: çå‹µè¨­è¨ˆæœ‰å•é¡Œ

# 3. æª¢æŸ¥ Episode é•·åº¦
max_episode_length = 500  # steps
# å¦‚æœå¤ªçŸ­: Agent æ²’æ™‚é–“å­¸ç¿’
# å¦‚æœå¤ªé•·: è¨“ç·´æ•ˆç‡ä½

# 4. æª¢æŸ¥ç’°å¢ƒæ•¸é‡
num_envs = 256  # SB3 æ¨è–¦å€¼
# å¦‚æœ < 64: æ¡æ¨£æ•ˆç‡ä½
# å¦‚æœ > 4096: å¯èƒ½ GPU è¨˜æ†¶é«”ä¸è¶³
```

---

## ğŸ”§ èª¿è©¦å·¥å…·èˆ‡æŒ‡ä»¤

### æŸ¥çœ‹è§€æ¸¬å€¼
```python
# åœ¨è¨“ç·´è…³æœ¬ä¸­æ·»åŠ 
obs, info = env.reset()
print("Observation shape:", obs.shape)
print("Observation range:", obs.min(), obs.max())
print("Per-dimension std:", obs.std(dim=0))
```

### æŸ¥çœ‹çå‹µåˆ†ä½ˆ
```python
# åœ¨ç’°å¢ƒä¸­æ·»åŠ æ—¥èªŒ
from ..mdp.rewards import _print_diagnostics

# åœ¨ reward term ä¸­å•Ÿç”¨
def your_reward(env):
    reward = calculate_reward(env)
    _print_diagnostics("your_reward", reward)
    return reward
```

### TensorBoard ç›£æ§
```bash
# å•Ÿå‹• TensorBoard
tensorboard --logdir ~/isaaclab/isaac-navigator-charge-sb3-v0/logs

# è§€å¯ŸæŒ‡æ¨™
- rollout/ep_rew_mean      # Episode å¹³å‡çå‹µ
- train/learning_rate      # å­¸ç¿’ç‡
- train/policy_loss        # Policy loss
- train/value_loss         # Value loss
- train/policy_entropy     # ç†µï¼ˆæ¢ç´¢ç¨‹åº¦ï¼‰
```

### éŒ„è£½è¨“ç·´éç¨‹
```bash
# å•Ÿç”¨å¯è¦–åŒ–è¨“ç·´ï¼ˆç§»é™¤ --headlessï¼‰
./isaaclab.sh -p scripts/reinforcement_learning/sb3/train_charge.py \
    --task Isaac-Navigation-Charge-SB3-v0 \
    --num_envs 64  # é™ä½ç’°å¢ƒæ•¸ä»¥é©é…æ¸²æŸ“

# éŒ„è£½å½±ç‰‡
--video  # è‡ªå‹•ä¿å­˜ MP4
```

---

## ğŸ“Š æ€§èƒ½åŸºæº–å€¼

### å¥åº·è¨“ç·´æŒ‡æ¨™

| æŒ‡æ¨™ | Phase 0 | Phase 1 | Phase 2 | Phase 3 |
|------|:-------:|:-------:|:-------:|:-------:|
| **æˆåŠŸç‡ (1M steps)** | >90% | >70% | >50% | >80% |
| **Episode é•·åº¦** | <200 | <300 | <400 | <500 |
| **å¹³å‡çå‹µ** | >100 | >50 | >20 | >80 |
| **Policy Entropy** | >0.5 | >0.3 | >0.2 | >0.4 |

### ç•°å¸¸è­¦å ±

```python
# è­¦å ±æ¢ä»¶
if ep_rew_mean < -100:
    print("ğŸš¨ çå‹µéä½ï¼æª¢æŸ¥çå‹µå‡½æ•¸è¨­è¨ˆ")

if policy_entropy < 0.1:
    print("ğŸš¨ ç†µéä½ï¼æ¢ç´¢ä¸è¶³ï¼Œå¢åŠ  ent_coef")

if success_rate < 0.1 and steps > 1e6:
    print("ğŸš¨ è¨“ç·´å¤±æ•—ï¼æª¢æŸ¥ç’°å¢ƒé…ç½®")

if episode_length > max_length * 0.95:
    print("âš ï¸ Episode éé•·ï¼å¯èƒ½éœ€è¦èª¿æ•´çµ‚æ­¢æ¢ä»¶")
```

---

## ğŸš¨ ç·Šæ€¥ä¿®å¾©æŒ‡ä»¤

```bash
# è¨“ç·´å´©æ½°æ™‚
# 1. æ®ºæ‰é€²ç¨‹
pkill -f train_charge.py

# 2. æ¸…ç†å¿«å–
rm -rf ~/.isaac/cache/*

# 3. é™ä½è¤‡é›œåº¦é‡å•Ÿ
./isaaclab.sh -p ... --num_envs 128  # é™ä½ç’°å¢ƒæ•¸

# 4. å¾æª¢æŸ¥é»æ¢å¾©
--load_path path/to/checkpoint.zip
```
